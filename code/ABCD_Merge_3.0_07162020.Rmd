---
title: "ABCD_Merge_3.0_07162020"
author: "Elizabeth McNeilly"
date: "7/16/2020"
output: html_document
---

#NOTE: 
- Change working directory to the location where the raw data is saved. 
#SETUP
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(plyr)
library(readr)

getwd()
# Set your data directory here.
data_directory <- "~/Desktop/Desktop_MacBookPro/Desktop_Folders/UO/ABCD_Workshop/Data_Analysis_3.0/test"

output_directory <- "~/Desktop/Desktop_MacBookPro/Desktop_Folders/UO/ABCD_Workshop/Data_Analysis_3.0/Output"

#Convert .txt files to .csv format. 
setwd(data_directory)
filelist = list.files(pattern = ".txt")
for (i in 1:length(filelist)){
    input<-filelist[i]
    output <- paste0(gsub("\\.txt$", "", input), ".csv")
    print(paste("Processing the file:", input))
    data = read.delim(input, header = TRUE)   
    write.table(data, file=output, sep=",", col.names=TRUE, row.names=FALSE)
}
```



#Separate Data Frames
```{r}

#Read in data in separate data frames.
setwd(data_directory)
csv_filelist = list.files(pattern = ".csv")

# This loop should work instead of the code below (commented out), but if not then use that instead.
# The function read_csv has a lot of output messages but it's the one that worked for all of them.
for (i in 1:length(csv_filelist)){
    filename <- csv_filelist[i]
    dataframe_name <- sub("abcd_", "", filename) # There's a prettier way to do this, but not all of them have "abcd" in the title, so will just do this for now.
    dataframe_name <- sub(".csv", "", dataframe_name)
    temporary_df <- read_csv(paste(data_directory,filename,sep="/")) # Create a temporary dataframe and then rename it based on the name of the file.
    assign(dataframe_name, temporary_df)
    print(paste("Created dataframe: ", dataframe_name))
}

# bisbas01 <- read.csv(paste(data_directory,"abcd_bisbas01.csv",sep="/"))
# 
# cbcls01 <- read.csv(paste(data_directory,"abcd_cbcls01.csv",sep="/"))
# 
# hsss01 <- read.csv(paste(data_directory,"abcd_hsss01.csv",sep="/"))
# 
# lpds01 <- read_csv(paste(data_directory,"abcd_lpds01.csv",sep="/"))
# 
# lt01 <- read.csv(paste(data_directory,"abcd_lt01.csv",sep="/"))
# 
# mid02 <- read.csv(paste(data_directory,"abcd_mid02.csv",sep="/"))
# 
# ppdms01 <- read.csv(paste(data_directory,"abcd_ppdms01.csv",sep="/"))
# 
# ypdms01 <- read.csv(paste(data_directory,"abcd_ypdms01.csv",sep="/"))
# 
# fhxp102 <- read.csv(paste(data_directory,"fhxp102.csv",sep="/"))
# 
# midaparc03 <- read.csv(paste(data_directory,"midaparc03.csv",sep="/"))
# 
# midaparcp203 <- read.csv(paste(data_directory,"midaparcp203.csv",sep="/"))
# 
# pdem02 <- read_csv(paste(data_directory,"pdem02.csv",sep="/"))
```


#Cleaning
```{r}
#THIS STEP WOULD BE MADE MORE EFFICIENT WITH A LOOP.
# The first row in each spreadsheet is the element description. Lets remove those for our data tables. This information is already present in the [ABCD Data Dictionaries](https://ndar.nih.gov/data_dictionary.html?source=ABCD%2BRelease%2B2.0&submission=ALL).


bisbas01 = bisbas01[-1,]
cbcls01 = cbcls01[-1,]
hsss01 = hsss01[-1,]
lpds01 = lpds01[-1,]
lt01 = lt01[-1,]
mid02 = mid02[-1,]
ppdms01 = ppdms01[-1,]
ypdms01 = ypdms01[-1,]
fhxp102 = fhxp102[-1,]
midaparc03 = midaparc03[-1,]
midaparcp203 = midaparcp203[-1,]
pdem02 = pdem02[-1,]

```


```{r}
# Drop columns introduced by NDA, and duplicated columns across data frames. Keep the Longitudinal Tracking data (lt01) as the anchor (Do not remove the columns from lt01.)


bisbas01 = bisbas01[,!(names(bisbas01) %in% c("collection_id", "collection_title", "subjectkey", "interview_age","interview_date","sex"))]

cbcls01 = cbcls01[,!(names(cbcls01) %in% c("collection_id", "collection_title", "subjectkey", "interview_age","interview_date","sex"))]

hsss01 = hsss01[,!(names(hsss01) %in% c("collection_id", "collection_title", "subjectkey", "interview_age","interview_date","sex"))]

lpds01 = lpds01[,!(names(lpds01) %in% c("collection_id", "collection_title", "subjectkey","interview_age","interview_date","sex"))]

mid02 = mid02[,!(names(mid02) %in% c("collection_id", "collection_title", "subjectkey", "interview_age","interview_date","sex"))]

ppdms01 = ppdms01[,!(names(ppdms01) %in% c("collection_id", "collection_title", "subjectkey", "interview_age","interview_date","sex"))]

ypdms01 = ypdms01[,!(names(ypdms01) %in% c("collection_id", "collection_title", "subjectkey", "interview_age","interview_date","sex"))]

fhxp102 = fhxp102[,!(names(fhxp102) %in% c("collection_id", "collection_title", "subjectkey", "interview_age","interview_date","sex"))]

midaparc03 = midaparc03[,!(names(midaparc03) %in% c("collection_id", "collection_title", "subjectkey","interview_age","interview_date","sex"))]

midaparcp203 = midaparcp203[,!(names(midaparcp203) %in% c("collection_id", "collection_title", "subjectkey", "interview_age","interview_date","sex"))]

pdem02 = pdem02[,!(names(pdem02) %in% c("collection_id", "collection_title", "subjectkey"))]

```


#MERGING
```{r}
#Merge the individual tables into a single spreadsheet. The following loop performs repeated merging operations between pairs of spreadsheets.


t2 = list(bisbas01, cbcls01, hsss01, lpds01, lt01, mid02, ppdms01, ypdms01, fhxp102, midaparc03, midaparcp203, pdem02)


while (length(t2) > 1 ) {
  print("iteration")
  access= seq(1,length(t2)-1,2)
  for (i in access) {
    bm = dim(t2[[i]])

    by.vars=c("src_subject_id","eventname")
    t2[[i]] = merge(t2[[i]], t2[[i+1]], by=by.vars, all=TRUE)

    print(paste("rows before: ", bm[1], dim(t2[[i+1]])[1], " rows after: ",dim(t2[[i]])[1], "indices: ",i,i+1," columns: ",bm[2],"+",dim(t2[[i+1]])[2], " = ",dim(t2[[i]])[2]))
  }
  # for odd number of instruments add the last spreadsheet back to the list
  if (length(t2) %% 2 != 0) access = append(access,length(t2))
  # reduce the list
  t2 = t2[access]
}
nda20 = t2[[1]]
nda20=nda20[,-which(grepl("dataset_id",colnames(nda20)))]



# Per the original merge script: we can sort the levels of eventname by timepoint.

nda20$eventname = factor(nda20$eventname, levels(nda20$eventname)[c(2,4,1,3)])


#NOTE: Per the original merge script: The nda data frame should contain 27,368 rows (baseline: 11,875; 6 month: 8,623; 1 year: 4,951; and 18 month: 1,919).


#As a last step we can save the data in R's native file format

saveRDS(nda20, paste(output_directory,"nda20.rds",sep="/"))

names.nda20=colnames(nda20)

save(file="names.nda20.RData",names.nda20)

# Optional -- save as .csv file as well

write.csv(nda20, paste(output_directory,"nda20.csv",sep="/"), row.names = FALSE)
```

#Read data back in
```{r}
# In order to read the data back into memory use:
nda20 = readRDS(paste(output_directory,"nda20.rds",sep="/"))

```


